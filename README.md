# GPT_small

This repository contains the code to implement a nanoGPT language model based on the tutorial by Andrej Karpathy [link to tutorial]. This is a simplified and educational implementation aiming to provide an accessible introduction to transformer-based models.

Features:

Trains a character-level GPT model on text data
Uses PyTorch and Transformers libraries
Includes basic training and evaluation scripts
Offers a clear and commented codebase for learning purposes

Prerequisites:

Python 3.6+
PyTorch
Transformers (>=4.7.0)
NumPy

Additional Notes:

Original Video: https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=8&t=4991s
